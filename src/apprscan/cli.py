"""Command-line interface for apprscan."""

from __future__ import annotations

import argparse
from pathlib import Path
from typing import Sequence

import pandas as pd

from . import __version__
from .distance import nearest_station_from_df
from .geocode import geocode_address
from . import normalize
from .normalize import normalize_companies
from .prh_client import fetch_companies
from .report import export_reports
from .stations import load_stations


def build_parser() -> argparse.ArgumentParser:
    parser = argparse.ArgumentParser(
        prog="apprscan",
        description="Apprenticeship employer scanner (PRH/YTJ + geokoodaus + raportointi).",
    )
    parser.add_argument("--version", action="version", version=f"%(prog)s {__version__}")

    subparsers = parser.add_subparsers(dest="command", required=False)

    # Jobs subcommand
    jobs_parser = subparsers.add_parser(
        "jobs",
        help="Hae työpaikat yritysten urasivuilta.",
        description="Crawlaa urasivuja (domain mapping + heuristiikat) ja normalisoi JobPosting-rivit.",
    )
    jobs_parser.add_argument("--companies", type=str, required=True, help="Yritystiedosto (xlsx/csv/parquet).")
    jobs_parser.add_argument("--domains", type=str, default=None, help="Domain mapping CSV (business_id,domain).")
    jobs_parser.add_argument("--out", type=str, default="out/jobs", help="Output-hakemisto.")
    jobs_parser.add_argument("--max-domains", type=int, default=300, help="Maksimi domainit per ajo.")
    jobs_parser.add_argument(
        "--max-pages-per-domain", type=int, default=30, help="Maksimi sivut per domain (guardrail)."
    )
    jobs_parser.set_defaults(func=jobs_command)

    run_parser = subparsers.add_parser(
        "run",
        help="Suorita haku ja raportointi.",
        description=(
            "Hakee PRH/YTJ:stä yrityksiä, geokoodaa osoitteet ja tuottaa raportit "
            "(Excel/GeoJSON/HTML)."
        ),
    )
    run_parser.add_argument(
        "--cities",
        type=str,
        help="Pilkuilla erotettu kaupunkilista (esim. Helsinki,Espoo,Vantaa,Lahti).",
    )
    run_parser.add_argument(
        "--radius-km",
        type=float,
        default=1.0,
        help="Suurin etäisyys (km) lähimmälle asemalle.",
    )
    run_parser.add_argument(
        "--main-business-line",
        type=str,
        default="",
        help="PRH mainBusinessLine -suodatin.",
    )
    run_parser.add_argument("--reg-start", type=str, default="", help="registrationDateStart (YYYY-MM-DD).")
    run_parser.add_argument("--reg-end", type=str, default="", help="registrationDateEnd (YYYY-MM-DD).")
    run_parser.add_argument("--max-pages", type=int, default=0, help="Maksimi sivut per kaupunki (0 = kaikki).")
    run_parser.add_argument(
        "--stations-file",
        type=str,
        default=None,
        help="Paikallinen asemadata CSV (station_name,lat,lon). Oletus: data/stations_fi.csv jos löytyy.",
    )
    run_parser.add_argument("--skip-geocode", action="store_true", help="Ohita geokoodaus (debug / nopea ajo).")
    run_parser.add_argument("--limit", type=int, default=0, help="Käsittele vain N ensimmäistä riviä (debug).")
    run_parser.add_argument(
        "--geocode-cache",
        type=str,
        default="data/geocode_cache.sqlite",
        help="SQLite-välimuisti geokoodaukselle.",
    )
    run_parser.add_argument(
        "--whitelist",
        type=str,
        default="",
        help="Pilkuilla eroteltu toimiala-whitelist (mainBusinessLine substring).",
    )
    run_parser.add_argument(
        "--blacklist",
        type=str,
        default="",
        help="Pilkuilla eroteltu toimiala-blacklist (hard fail).",
    )
    run_parser.add_argument(
        "--include-excluded",
        action="store_true",
        help="Sisällytä poissuljetut rivit Exceliin (Excluded-välilehti).",
    )
    run_parser.add_argument(
        "--employee-csv",
        type=str,
        default=None,
        help="Työntekijämäärä-enrichment CSV (business_id, employee_count/employee_band).",
    )
    run_parser.add_argument(
        "--out",
        type=str,
        default="out",
        help="Tulosten hakemisto (Excel/GeoJSON/HTML).",
    )
    run_parser.set_defaults(func=run_command)

    return parser


def run_command(args: argparse.Namespace) -> int:
    cities = [c.strip() for c in (args.cities or "").split(",") if c.strip()]
    if not cities:
        print("Anna vähintään yksi kaupunki --cities parametrilla.")
        return 1

    all_rows = []
    for city in cities:
        rows = fetch_companies(
            location=city,
            main_business_line=args.main_business_line or None,
            reg_start=args.reg_start or None,
            reg_end=args.reg_end or None,
            max_pages=args.max_pages,
        )
        for r in rows:
            r["_source_city"] = city
        all_rows.extend(rows)

    if args.limit:
        all_rows = all_rows[: args.limit]

    print(f"Haettu rivejä: {len(all_rows)}")
    df = normalize_companies(all_rows)
    if df.empty:
        print("Ei rivejä käsiteltäväksi.")
        return 0

    built_addresses = int(df["full_address"].astype(bool).sum())
    print(f"Muodostetut osoitteet: {built_addresses} / {len(df)}")

    if args.skip_geocode:
        df["lat"] = None
        df["lon"] = None
        df["geocode_provider"] = None
        df["geocode_cache_hit"] = None
    else:
        lats = []
        lons = []
        providers = []
        cache_hits = []
        for addr in df["full_address"]:
            lat, lon, provider, cached = geocode_address(addr, cache_path=Path(args.geocode_cache))
            lats.append(lat)
            lons.append(lon)
            providers.append(provider)
            cache_hits.append(cached)
        df["lat"] = lats
        df["lon"] = lons
        df["geocode_provider"] = providers
        df["geocode_cache_hit"] = cache_hits

    df = normalize.deduplicate_companies(df)

    stations_df = None
    try:
        stations_df = load_stations(use_local=True, path=args.stations_file)
    except Exception as exc:  # pragma: no cover - defensive logging
        print(f"Asemadatan lataus epäonnistui: {exc}")

    nearest_names = []
    nearest_dists = []
    if stations_df is not None and not df[["lat", "lon"]].isna().all().all():
        for _, row in df.iterrows():
            if pd.isna(row.get("lat")) or pd.isna(row.get("lon")):
                nearest_names.append("")
                nearest_dists.append(None)
                continue
            name, dist = nearest_station_from_df(float(row["lat"]), float(row["lon"]), stations_df)
            nearest_names.append(name)
            nearest_dists.append(dist)
    else:
        nearest_names = [""] * len(df)
        nearest_dists = [None] * len(df)

    df["nearest_station"] = nearest_names
    df["distance_km"] = nearest_dists

    from .filters import exclude_company, industry_pass
    from .scoring import score_company
    from .storage import load_employee_enrichment

    excluded_flags = []
    excluded_reasons = []
    industry_whitelist_hit = []
    industry_blacklist_hit = []
    industry_reason_col = []

    wl = [s.strip() for s in (args.whitelist or "").split(",") if s.strip()]
    bl = [s.strip() for s in (args.blacklist or "").split(",") if s.strip()]

    for _, row in df.iterrows():
        excl, reason = exclude_company(row.to_dict())
        excluded_flags.append(excl)
        excluded_reasons.append(reason)
        ind_pass, ind_reason, hard_fail = industry_pass(row.to_dict(), wl, bl)
        industry_whitelist_hit.append(ind_reason and ind_reason.startswith("whitelist"))
        industry_blacklist_hit.append(ind_reason and ind_reason.startswith("blacklist"))
        if hard_fail and not excl and ind_reason:
            excl = True
            reason = ind_reason
            excluded_flags[-1] = excl
            excluded_reasons[-1] = reason
        industry_reason_col.append(ind_reason)

    df["excluded_reason"] = excluded_reasons

    # Employee enrichment
    enrichment = {}
    if args.employee_csv:
        try:
            enrichment = load_employee_enrichment(args.employee_csv)
        except Exception as exc:  # pragma: no cover - defensive logging
            print(f"Työntekijäenrichment epäonnistui: {exc}")

    employee_counts = []
    employee_bands = []
    employee_sources = []
    employee_gate = []
    for _, row in df.iterrows():
        bid = str(row.get("business_id") or "")
        if bid and bid in enrichment:
            enr = enrichment[bid]
            cnt = enr.get("employee_count")
            band = enr.get("employee_band")
            source = enr.get("employee_source", "csv")
            employee_counts.append(cnt if pd.notna(cnt) else None)
            employee_bands.append(band if pd.notna(band) else None)
            employee_sources.append(source)
            if cnt is not None and pd.notna(cnt):
                try:
                    gate = "pass" if float(cnt) >= 5 else "fail"
                except (TypeError, ValueError):
                    gate = "unknown"
            elif band:
                gate = "unknown"
            else:
                gate = "unknown"
        else:
            employee_counts.append(None)
            employee_bands.append(None)
            employee_sources.append(None)
            gate = "unknown"
        employee_gate.append(gate)
        if gate == "fail":
            reason = "employee_lt_5"
            excluded_reasons[df.index.get_loc(row.name)] = reason
            excluded_flags[df.index.get_loc(row.name)] = True

    df["employee_count"] = employee_counts
    df["employee_band"] = employee_bands
    df["employee_source"] = employee_sources
    df["employee_gate"] = employee_gate

    scores = []
    score_reasons = []
    for excl, row, wl_hit, bl_hit in zip(excluded_flags, df.iterrows(), industry_whitelist_hit, industry_blacklist_hit):
        _, r = row
        s, reasons = score_company(
            r.to_dict(),
            radius_km=args.radius_km,
            industry_whitelist_hit=bool(wl_hit),
            industry_blacklist_hit=bool(bl_hit),
            excluded=bool(excl),
        )
        scores.append(s)
        score_reasons.append(reasons)
    df["score"] = scores
    df["score_reasons"] = score_reasons

    df["excluded_reason"] = excluded_reasons

    shortlist = df[df["excluded_reason"].isna()]
    shortlist = shortlist.sort_values(["score", "distance_km"], ascending=[False, True]).reset_index(drop=True)
    shortlist["rank"] = shortlist.index + 1

    excluded_df = None
    if args.include_excluded:
        excluded_df = df[df["excluded_reason"].notna()].copy()

    export_reports(shortlist, args.out, excluded=excluded_df)
    print(f"Shortlist: {len(shortlist)}; excluded: {len(df) - len(shortlist)}; raportit: {args.out}")
    return 0


def jobs_command(args: argparse.Namespace) -> int:
    from .jobs import crawl_jobs_for_companies, load_domain_mapping, write_jobs_outputs

    # Load companies
    companies_path = Path(args.companies)
    if not companies_path.exists():
        print(f"Companies file not found: {companies_path}")
        return 1
    if companies_path.suffix.lower() in [".xlsx", ".xls"]:
        companies_df = pd.read_excel(companies_path)
    elif companies_path.suffix.lower() in [".csv"]:
        companies_df = pd.read_csv(companies_path)
    elif companies_path.suffix.lower() in [".parquet"]:
        companies_df = pd.read_parquet(companies_path)
    else:
        print("Unsupported companies file format (use xlsx/csv/parquet).")
        return 1

    domain_map = load_domain_mapping(args.domains)
    jobs_df, stats_df = crawl_jobs_for_companies(
        companies_df,
        domain_map,
        max_domains=args.max_domains,
        max_pages_per_domain=args.max_pages_per_domain,
    )
    write_jobs_outputs(jobs_df, stats_df, args.out)
    print(f"Jobs found: {len(jobs_df)}; stats rows: {len(stats_df)}; output: {args.out}")
    return 0


def main(argv: Sequence[str] | None = None) -> int:
    parser = build_parser()
    args = parser.parse_args(argv)
    if not hasattr(args, "func"):
        parser.print_help()
        return 0
    return args.func(args)
